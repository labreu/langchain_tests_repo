{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have some text documents (PDF, blog, Notion pages, etc.) and want to ask questions related to the contents of those documents.\n",
    "\n",
    "LLMs, given their proficiency in understanding text, are a great tool for this.\n",
    "\n",
    "In this walkthrough we'll go over how to build a question-answering over documents application using LLMs.\n",
    "\n",
    "Two very related use cases which we cover elsewhere are:\n",
    "\n",
    "QA over structured data (e.g., SQL)\n",
    "QA over code (e.g., Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://python.langchain.com/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg\" width=\"1000\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader, CSVLoader  #more integrations here https://integrations.langchain.com/\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load documents\n",
    "loader = WebBaseLoader(['https://python.langchain.com/docs/get_started/introduction', 'https://python.langchain.com/docs/integrations/document_loaders/pandas_dataframe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents\n",
    "# Context-aware splitters keep the location (\"context\") of each split in the original Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "splits = text_splitter.split_documents(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Introduction | ğŸ¦œï¸�ğŸ”— Langchain', metadata={'source': 'https://python.langchain.com/docs/get_started/introduction', 'title': 'Introduction | ğŸ¦œï¸�ğŸ”— Langchain', 'description': 'LangChain is a framework for developing applications powered by language models. It enables applications that:', 'language': 'en'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store splits\n",
    "#https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html\n",
    "#https://huggingface.co/models?library=sentence-transformers&sort=downloads\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "#pip install urllib3<2  https://stackoverflow.com/questions/76414514/cannot-import-name-default-ciphers-from-urllib3-util-ssl-on-aws-lambda-us\n",
    "emb = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "#emb = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=emb)  #can also use local FAISS db\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever configs:\n",
    "\n",
    "* Retrieve more documents with higher diversity- useful if your dataset has many similar documents\n",
    "> docsearch.as_retriever(search_type=\"mmr\", search_kwargs={'k': 6, 'lambda_mult': 0.25})\n",
    "\n",
    "* Fetch more documents for the MMR algorithm to consider, but only return the top 5\n",
    "> docsearch.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5, 'fetch_k': 50})\n",
    "\n",
    "* Only retrieve documents that have a relevance score above a certain threshold\n",
    "> docsearch.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8})\n",
    "\n",
    "* Only get the single most similar document from the dataset\n",
    "> docsearch.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "* Use a filter to only retrieve documents from a specific paper\n",
    "> docsearch.as_retriever(search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "# https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "\n",
    "# You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "# Question: {question} \n",
    "# Context: {context} \n",
    "# Answer:\n",
    "\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain is a decentralized blockchain platform that aims to provide language services and solutions. It utilizes blockchain technology to create a secure and transparent ecosystem for language-related transactions, such as translation, interpretation, and language learning. Langchain aims to connect language service providers and users directly, eliminating intermediaries and reducing costs. It also offers features like smart contracts, reputation systems, and payment solutions to ensure efficient and reliable language services.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('what is langchain?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain \n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | rag_prompt \n",
    "    | llm \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is a framework for developing applications powered by language models. It enables applications that are context-aware and can reason based on provided context. LangChain provides components that are modular and easy-to-use for working with language models.')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke('what is langchain?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing another LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n"
     ]
    }
   ],
   "source": [
    "# open ai works fine!\n",
    "#llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# network error\n",
    "#from langchain.llms import HuggingFaceEndpoint\n",
    "# endpoint_url = (\n",
    "#                 \"https://abcdefghijklmnop.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "#             )\n",
    "# hf = HuggingFaceEndpoint(\n",
    "#     endpoint_url=endpoint_url,\n",
    "# )\n",
    "\n",
    "#takes long time to load\n",
    "#from langchain.llms import HuggingFaceHub  # examples https://python.langchain.com/docs/integrations/llms/huggingface_hub.html\n",
    "#repo_id = \"databricks/dolly-v2-3b\"         # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "#repo_id = \"Qwen/Qwen-7B\"\n",
    "#hf = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64})   #\"gpt2\" very bad\n",
    "\n",
    "from langchain.llms import GPT4All\n",
    "model = GPT4All(model=\"./wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\", n_threads=8) #https://gpt4all.io/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLangChain (LNG) is a decentralized platform for creating, managing and monetizing multilingual content. The LangChain ecosystem enables users to create, translate, edit, store, manage, and distribute multilingual content in an efficient, transparent, and secure manner.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model('what is langchain?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LangChain is a framework for developing applications powered by language models, enabling context-aware and reasonable solutions with components that are modular, easy to use, and supported by various implementations.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG chain with LLM\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | rag_prompt \n",
    "    | model \n",
    ")\n",
    "\n",
    "rag_chain.invoke('what is langchain?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another approach with new lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a framework for developing applications powered by language models. It enables applications to be context-aware and rely on a language model to reason and provide responses based on provided context. LangChain provides components that are modular and easy-to-use for working with language models.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt}\n",
    ")\n",
    "question = 'what is langchain?'\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
